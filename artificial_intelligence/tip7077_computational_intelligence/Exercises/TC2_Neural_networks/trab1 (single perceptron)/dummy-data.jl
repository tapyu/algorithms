using FileIO, JLD2, Random, LinearAlgebra, Plots, LaTeXStrings
Î£=sum

### dataset 02 - artificial dataset ###

## algorithm parameters hyperparameters
Ïƒâ‚“ = .1 # signal standard deviation
N = 40 # number of instances
Nâ‚ = 2 # number of attributes (not includes the bias)
Náµ£ = 20 # number of realizations
Nâ‚‘ = 100 # number of epochs
Nâ‚œáµ£â‚™ = 80 # % percentage of instances for the train dataset
Nâ‚œâ‚›â‚œ = 20 # % percentage of instances for the test dataset
Î± = 0.01 # learning step

## useful functions
function shuffle_dataset(ğ—, ğ)
    shuffle_indices = Random.shuffle(1:size(ğ—,2))
    return ğ—[:, shuffle_indices], ğ[shuffle_indices]
end

function train(ğ—, ğ, ğ°, is_training_accuracy=true)
    Ï† = uâ‚â‚™â‚ -> uâ‚â‚™â‚>0 ? 1 : 0 # McCulloch and Pitts's activation function (step function)
    Nâ‚‘ = 0 # number of errors â¡ misclassifications
    for (ğ±â‚â‚™â‚, dâ‚â‚™â‚) âˆˆ zip(eachcol(ğ—), ğ)
        Î¼â‚â‚™â‚ = dot(ğ±â‚â‚™â‚,ğ°) # inner product
        yâ‚â‚™â‚ = Ï†(Î¼â‚â‚™â‚) # for the training phase, you do not pass yâ‚â‚™â‚ to a harder decisor (the McCulloch and Pitts's activation function) since you are in intended to classify yâ‚â‚™â‚. Rather, you are interested in updating ğ° (??? TODO)
        eâ‚â‚™â‚ = dâ‚â‚™â‚ - yâ‚â‚™â‚
        ğ° += Î±*eâ‚â‚™â‚*ğ±â‚â‚™â‚

        # this part is optional: only if it is interested in seeing the accuracy evolution of the training dataset throughout the epochs
        Nâ‚‘ = eâ‚â‚™â‚==0 ? Nâ‚‘ : Nâ‚‘+1
    end
    if is_training_accuracy
        accuracy = (length(ğ)-Nâ‚‘)/length(ğ) # accuracy for this epoch
        return ğ°, accuracy
    else
        return ğ°
    end
end

function test(ğ—, ğ, ğ°, is_confusion_matrix=false)
    Ï† = uâ‚â‚™â‚ -> uâ‚â‚™â‚>0 ? 1 : 0 # McCulloch and Pitts's activation function (step function)
    ğ² = rand(length(ğ)) # vector of predictions for confusion matrix
    Nâ‚‘ = 0
    for (n, (ğ±â‚â‚™â‚, dâ‚â‚™â‚)) âˆˆ enumerate(zip(eachcol(ğ—), ğ))
        Î¼â‚â‚™â‚ = ğ±â‚â‚™â‚â‹…ğ° # inner product
        yâ‚â‚™â‚ = Ï†(Î¼â‚â‚™â‚) # for the single-unit perceptron, yâ‚â‚™â‚ âˆˆ {0,1}. Therefore, it is not necessary to pass yâ‚â‚™â‚ to a harder decisor since Ï†(â‹…) already does this job
        ğ²[n] = yâ‚â‚™â‚

        Nâ‚‘ = yâ‚â‚™â‚==dâ‚â‚™â‚ ? Nâ‚‘ : Nâ‚‘+1
    end
    if !is_confusion_matrix
        accuracy = (length(ğ)-Nâ‚‘)/length(ğ)
        return accuracy # return the accuracy for this realization
    else
        return Int.(ğ²) # return the errors over the instances to plot the confusion matrix
    end
end

## generate dummy data
ğ—â‚ = [Ïƒâ‚“*randn(10)'; Ïƒâ‚“*randn(10)']
ğ—â‚‚ = [Ïƒâ‚“*randn(10)'.+1; Ïƒâ‚“*randn(10)']
ğ—â‚ƒ = [Ïƒâ‚“*randn(10)'; Ïƒâ‚“*randn(10)'.+1]
ğ—â‚„ = [Ïƒâ‚“*randn(10)'.+1; Ïƒâ‚“*randn(10)'.+1]

ğ— = [fill(-1,N)'; [ğ—â‚ ğ—â‚‚ ğ—â‚ƒ ğ—â‚„]]
ğ = [zeros(30);ones(10)]

## init
accâ‚œâ‚›â‚œ = fill(NaN, Náµ£) # vector of accuracies for test dataset (to compute the final statistics)
figs_surface = Array{Plots.Plot{Plots.GRBackend},1}[]
figs_training_accuracy = Array{Plots.Plot{Plots.GRBackend},1}[]
for náµ£ âˆˆ 1:Náµ£
    # initialize
    global ğ° = randn(Nâ‚+1)
    accâ‚œáµ£â‚™ = fill(NaN, Nâ‚‘) # vector of accuracies for train dataset (to see its evolution during training phase)

    # prepare the data!
    global  ğ—, ğ
    ğ—, ğ = shuffle_dataset(ğ—, ğ)
    # hould-out
    global ğ—â‚œáµ£â‚™ = ğ—[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    global ğâ‚œáµ£â‚™ = ğ[1:(N*Nâ‚œáµ£â‚™)Ã·100]
    global ğ—â‚œâ‚›â‚œ = ğ—[:,length(ğâ‚œáµ£â‚™)+1:end]
    global ğâ‚œâ‚›â‚œ = ğ[length(ğâ‚œáµ£â‚™)+1:end]

    # train!
    for nâ‚‘ âˆˆ 1:Nâ‚‘
        ğ°, accâ‚œáµ£â‚™[nâ‚‘] = train(ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™, ğ°)
        ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™ = shuffle_dataset(ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™)
    end
    # test!
    accâ‚œâ‚›â‚œ[náµ£] = test(ğ—â‚œâ‚›â‚œ, ğâ‚œâ‚›â‚œ, ğ°) # accuracy for this realization

    # make plots!
    # accuracy training dataset x Epochs
    local fig = plot(accâ‚œáµ£â‚™, label="", xlabel=L"Epochs", ylabel="Accuracy", linewidth=2)
    push!(figs_training_accuracy, [fig])

    # decision surface
    Ï† = uâ‚â‚™â‚ -> uâ‚â‚™â‚â‰¥0 ? 1 : 0 # activation function of the single-unit perceptron
    xâ‚_range = floor(minimum(ğ—[2,:])):.1:ceil(maximum(ğ—[2,:]))
    xâ‚‚_range = floor(minimum(ğ—[3,:])):.1:ceil(maximum(ğ—[3,:]))
    y(xâ‚, xâ‚‚) = Ï†(dot([-1, xâ‚, xâ‚‚], ğ°))
    fig = surface(xâ‚_range, xâ‚‚_range, y, camera=(60,40,0), xlabel = L"x_1", ylabel = L"x_2", zlabel="decision surface")
    push!(figs_surface, [fig])

    # train and desired label 
    scatter!(ğ—â‚œáµ£â‚™[2,ğâ‚œáµ£â‚™.==1], ğ—â‚œáµ£â‚™[3,ğâ‚œáµ£â‚™.==1], ones(length(filter(x->x==1, ğâ‚œáµ£â‚™))), markershape = :hexagon, markersize = 4, markeralpha = 0.6, markercolor = :green, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "desired class train set")
    
    # test and desired label 
    scatter!(ğ—â‚œâ‚›â‚œ[2,ğâ‚œâ‚›â‚œ.==1], ğ—â‚œâ‚›â‚œ[3,ğâ‚œâ‚›â‚œ.==1], ones(length(filter(x->x==1, ğâ‚œâ‚›â‚œ))), markershape = :cross, markersize = 4, markeralpha = 0.6, markercolor = :green, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "desired class test set")

    # train and not desired label 
    scatter!(ğ—â‚œáµ£â‚™[2,ğâ‚œáµ£â‚™.==0], ğ—â‚œáµ£â‚™[3,ğâ‚œáµ£â‚™.==0], zeros(length(filter(x->x==0, ğâ‚œáµ£â‚™))), markershape = :hexagon, markersize = 4, markeralpha = 0.6, markercolor = :red, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "not desired class train set")

    # test and not desired label 
    scatter!(ğ—â‚œâ‚›â‚œ[2,ğâ‚œâ‚›â‚œ.==0], ğ—â‚œâ‚›â‚œ[3,ğâ‚œâ‚›â‚œ.==0], zeros(length(filter(x->x==0, ğâ‚œâ‚›â‚œ))), markershape = :cross, markersize = 4, markeralpha = 0.6, markercolor = :red, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "not desired class test set")
    title!("Decision surface for the desired class")
end

# analyze the accuracy statistics of each independent realization
aÌ„cÌ„cÌ„ = Î£(accâ‚œâ‚›â‚œ)/Náµ£ # Mean
ğ”¼accÂ² = Î£(accâ‚œâ‚›â‚œ.^2)/Náµ£
Ïƒacc = sqrt.(ğ”¼accÂ² .- aÌ„cÌ„cÌ„.^2) # standard deviation

println("Mean accuracy: $(aÌ„cÌ„cÌ„)")
println("Standard deviation: $(Ïƒacc)")

# find closest surface
i = 1
accâ‚œâ‚›â‚œ_closest_to_accuracy = accâ‚œâ‚›â‚œ[1]
for náµ£ âˆˆ 2:Náµ£
    if accâ‚œâ‚›â‚œ[náµ£] < accâ‚œâ‚›â‚œ_closest_to_accuracy
        global accâ‚œâ‚›â‚œ_closest_to_accuracy = accâ‚œâ‚›â‚œ[náµ£]
        global i = náµ£
    end
end

# plot the training set accuracy by epoch for the realization whose test dataset accuracy is closest to the mean accuracy
display(figs_training_accuracy[i][1])
savefig(figs_training_accuracy[i][1], "trab1 (single-unit perceptron)/figs/training dataset accuracy for realization $(i) (closest to the mean acc).png")
# plot surface decision for  the realization whose test dataset accuracy is closest to the mean accuracy
display(figs_surface[i][1])
savefig(figs_surface[i][1], "trab1 (single-unit perceptron)/figs/decision-surface-for-dummy-data.png")