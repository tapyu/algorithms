using LinearAlgebra, DSP, Plots, LaTeXStrings
Î£ = sum

N = 200 # number of samples
ğš = randn(N) # a(n) ~ N(0, 1)
ÏƒÂ² = 1 # variance of a
ğ = ğš # d(n) = a(n)
ğ¡ = [1, 1.6] # channel filter coefficients
Î¼ = .1 # step learning

ğ‘ = [3.56 1.6
1.6 3.56]
ğ© = [1.6, 0]
ğ°â‚’ = inv(ğ‘)*ğ© # Wiener solution
J(wâ‚€, wâ‚) = ÏƒÂ² - 2ğ©â‹…[wâ‚€; wâ‚] + [wâ‚€; wâ‚]' * ğ‘ * [wâ‚€; wâ‚] # cost-function

# channel output
ğ± = rand(N)
for n âˆˆ 2:N
    ğšâ‚â‚™â‚ = [ğš[n], ğš[n-1]] # input vector at the instant n
    ğ±[n] = ğ¡ â‹… ğšâ‚â‚™â‚ # x(n)
end

# steepest descent
ğ°â‚â‚™â‚ = zeros(2) # initial guess of the coefficient vector
ğ– = rand(2, N) # save the coefficient vector evolution
ğ–[:,1] = ğ°â‚â‚™â‚ # save initial position
ğ² = rand(N) # output signal
ğ”¼eÂ² = zeros(N) # error signal
for n âˆˆ 2:N
    ğ±â‚â‚™â‚ = [ğ±[n], ğ±[n-1]] # input vector at the instant n
    ğ²[n] = ğ°â‚â‚™â‚ â‹… ğ±â‚â‚™â‚ # y(n)
    ğ â‚â‚™â‚ = -2ğ© + 2ğ‘*ğ°â‚â‚™â‚ # deterministic gradient
    global ğ°â‚â‚™â‚ -= Î¼*ğ â‚â‚™â‚
    global ğ–[:,n] = ğ°â‚â‚™â‚
    global ğ”¼eÂ²[n] = ((n-2)*ğ”¼eÂ²[n-1] + (ğ[n] - ğ²[n])^2)/(n-1)
end
p1 = plot([ğ² ğ], title="Steepest descent "*L"(\mathbf{w}(n) = \mathbf{w}(n) - \mu \mathbf{g}(n))", label=[L"y(n)" L"d(n)"], legend=:bottomleft)
e1 = plot(ğ”¼eÂ², title="MSE of the Steepest descent", label=L"\mathbb{E}[e^2(n)]")
fig = contour(-.5:.05:1.5, -1.5:.05:1.5, J) # add the cost function contour to the last plot
scatter!([ğ°â‚’[1]], [ğ°â‚’[2]], markershape= :circle, color= :red, markersize = 6, label = "Wiener solution", title="Steepest descent coefficients over the contours") # ! make plots the scatter on the same axis
plot!(ğ–[1,:], ğ–[2,:], xlabel=L"w_1", ylabel=L"w_2", label="", markershape=:xcross, color=:blue)
savefig(fig, "list3/figs/q4c_steepest_descent.png")

# Newton's algorithm
ğ°â‚â‚™â‚ = zeros(2) # initial guess of the coefficient vector
ğ– = rand(2, N) # save the coefficient vector evolution
ğ–[:,1] = ğ°â‚â‚™â‚ # save initial position
ğ² = rand(N) # output signal
ğ”¼eÂ² = zeros(N) # error signal
ğ‡ = 2ğ‘ # the Hessian
for n âˆˆ 2:N
    ğ±â‚â‚™â‚ = [ğ±[n], ğ±[n-1]] # input vector at the instant n
    ğ²[n] = ğ°â‚â‚™â‚ â‹… ğ±â‚â‚™â‚ # y(n)
    ğ â‚â‚™â‚ = -2ğ© + 2ğ‘*ğ°â‚â‚™â‚ # deterministic gradient
    Î”ğ°â‚â‚™â‚Šâ‚â‚ = -inv(ğ‡)*ğ â‚â‚™â‚
    global ğ°â‚â‚™â‚ += Î”ğ°â‚â‚™â‚Šâ‚â‚
    global ğ–[:,n] = ğ°â‚â‚™â‚
    global ğ”¼eÂ²[n] = ((n-2)*ğ”¼eÂ²[n-1] + (ğ[n] - ğ²[n])^2)/(n-1)
end
p2 = plot([ğ² ğ], title="Newton's algorithm "*L"(\mathbf{w}(n) = \mathbf{w}(n) - \mu \mathbf{H}^{-1}\mathbf{g}(n))" , label=[L"y(n)" L"d(n)"], legend=:bottomleft)
e2 = plot(ğ”¼eÂ², title="MSE of the Newton's algorithm", label=L"\mathbb{E}[e^2(n)]")
fig = contour(-.5:.05:1.5, -1.5:.05:1.5, J) # add the cost function contour to the last plot
scatter!([ğ°â‚’[1]], [ğ°â‚’[2]], markershape= :circle, color= :red, markersize = 6, label = "Wiener solution", title="Newton's algorithm coefficients over the contours") # ! make plots the scatter on the same axis
plot!(ğ–[1,:], ğ–[2,:], xlabel=L"w_1", ylabel=L"w_2", label="", markershape=:xcross, color=:blue)
savefig(fig, "list3/figs/q4c_newton_algorithm.png")

# Least-Mean Squares (LMS) algorithm
Î¼ = .015 # step learning
global ğ°â‚â‚™â‚ = zeros(2) # initial guess of the coefficient vector
ğ– = rand(2, N) # save the coefficient vector evolution
ğ–[:,1] = ğ°â‚â‚™â‚ # save initial position
ğ² = rand(N) # output signal
ğ”¼eÂ² = zeros(N) # error signal
for n âˆˆ 2:N
    ğ±â‚â‚™â‚ = [ğ±[n], ğ±[n-1]] # input vector at the instant n
    ğ²[n] = ğ°â‚â‚™â‚ â‹… ğ±â‚â‚™â‚ # y(n)
    eâ‚â‚™â‚ = ğ[n] - ğ²[n]
    ğ Ì‚â‚â‚™â‚ = -2eâ‚â‚™â‚*ğ±â‚â‚™â‚ # stochastic gradient
    global ğ°â‚â‚™â‚ -= Î¼*ğ Ì‚â‚â‚™â‚
    global ğ–[:,n] = ğ°â‚â‚™â‚
    global ğ”¼eÂ²[n] = ((n-2)*ğ”¼eÂ²[n-1] + eâ‚â‚™â‚^2)/(n-1)
end
p3 = plot([ğ² ğ], title="LMS algorithm "*L"(\mathbf{w}(n) = \mathbf{w}(n) + 2\mu e(n)\mathbf{x}(n))", label=[L"y(n)" L"d(n)"], legend=:bottomleft)
e3 = plot(ğ”¼eÂ², title="MSE of the LMS algorithm", label=L"\mathbb{E}[e^2(n)]")
fig = contour(-.5:.05:1, -1.5:.05:1, J) # add the cost function contour to the last plot
scatter!([ğ°â‚’[1]], [ğ°â‚’[2]], markershape= :circle, color= :red, markersize = 6, label = "Wiener solution", title="LMS algorithm coefficients over the contours") # ! make plots the scatter on the same axis
plot!(ğ–[1,:], ğ–[2,:], xlabel=L"w_1", ylabel=L"w_2", label="", markershape=:xcross, color=:blue)
savefig(fig, "list3/figs/q4c_lms_algorithm.png")

# normalized LMS algorithm
Î¼ = .5 # step learning
ğ°â‚â‚™â‚ = zeros(2) # initial guess of the coefficient vector
ğ– = rand(2, N) # save the coefficient vector evolution
ğ–[:,1] = ğ°â‚â‚™â‚ # save initial position
Î³ = 20 # Normalized LMS hyperparameter
ğ² = rand(N) # output signal
ğ”¼eÂ² = zeros(N) # error signal
for n âˆˆ 2:N
    ğ±â‚â‚™â‚ = [ğ±[n], ğ±[n-1]] # input vector at the instant n
    ğ²[n] = ğ°â‚â‚™â‚ â‹… ğ±â‚â‚™â‚ # y(n)
    eâ‚â‚™â‚ = ğ[n] - ğ²[n]
    ğ Ì‚â‚â‚™â‚ = -2eâ‚â‚™â‚*ğ±â‚â‚™â‚ # stochastic gradient
    global ğ°â‚â‚™â‚ -= Î¼*ğ Ì‚â‚â‚™â‚/(ğ±â‚â‚™â‚â‹…ğ±â‚â‚™â‚ + Î³)
    global ğ–[:,n] = ğ°â‚â‚™â‚
    global ğ”¼eÂ²[n] = ((n-2)*ğ”¼eÂ²[n-1] + eâ‚â‚™â‚^2)/(n-1)
end
p4 = plot([ğ² ğ], title="Normalized LMS algorithm "*L"\left(\mathbf{w}(n) = \mathbf{w}(n) + \frac{2\mu e(n)\mathbf{x}(n)}{\mathbf{x}^\mathrm{T}(n)\mathbf{x}(n) + \gamma}\right)", label=[L"y(n)" L"d(n)"], legend=:bottomleft)
e4 = plot(ğ”¼eÂ², title="MSE of the normalized LMS algorithm", label=L"\mathbb{E}[e^2(n)]")
fig = contour(-.5:.05:1.5, -1.5:.05:1.5, J) # add the cost function contour to the last plot
scatter!([ğ°â‚’[1]], [ğ°â‚’[2]], markershape= :circle, color= :red, markersize = 6, label = "Wiener solution", title="Normalized LMS algorithm coefficients over the contours") # ! make plots the scatter on the same axis
plot!(ğ–[1,:], ğ–[2,:], xlabel=L"w_1", ylabel=L"w_2", label="", markershape=:xcross, color=:blue)
savefig(fig, "list3/figs/q4c_normalized_lms_algorithm.png")

# final plots
fig = plot(p1, p2, p3, p4, layout=(4,1), size=(1200,800))
savefig(fig, "list3/figs/q4_all_filter_output.png")

fig = plot(e1, e2, e3, e4, layout=(4,1), size=(1200,800))
savefig(fig, "list3/figs/q4-error-evolution.png")