using CSV, DataFrames, Random, DataStructures, Plots
Î£=sum
âŠ™ = .* # Hadamard product

df = CSV.read("../Datasets/Electric Motor Temperature [kaggle]/measures_v2.csv", DataFrame)
df = df[df.profile_id.==17, :]

## load the dataset!
# coolant           â¡ Coolant temperature (in Â°C)
# stator_winding    â¡ Stator winding temperature (in Â°C) measured with thermocouples
# stator_tooth      â¡ Stator tooth temperature (in Â°C) measured with thermocouples
# pm                â¡ Permanent magnet temperature (in Â°C) measured with thermocouples and transmitted wirelessly via a thermography unit.
# stator_yoke       â¡ Stator yoke temperature (in Â°C) measured with thermocouples
ğƒ = [df.coolant df.stator_winding df.stator_tooth df.pm df.stator_yoke]'

# motor_speed   â¡ Motor speed (in rpm)
# torque        â¡ Motor torque (in Nm)
# u_q           â¡ Voltage q-component measurement in dq-coordinates (in V)
# u_d           â¡ Voltage d-component measurement in dq-coordinates
# i_q           â¡ Current q-component measurement in dq-coordinates
# i_d           â¡ Current d-component measurement in dq-coordinates
# ambient       â¡ Ambient temperature (in Â°C)
ğ— = [df.torque df.motor_speed df.u_q df.u_d df.i_q df.i_d df.ambient]'

## algorithm parameters and hyperparameters
N = size(ğ—, 2) # number of instances (it is analyzed only one session since the dataset has more than one million samples -_-)
Nâ‚œáµ£â‚™ = 80 # % percentage of instances for the train dataset
Nâ‚œâ‚›â‚œ = 20 # % percentage of instances for the test dataset
Nâ‚ = size(ğ—, 1) # number of number of attributes
Náµ£ = 20 # number of realizations
Nâ‚‘ = 100 # number of epochs
mâ‚‚ = size(ğƒ, 1) # number of perceptrons (neurons) of the output layer â¡ number of variables for regression
Î· = 0.1 # learning step

## Standardize dataset (Preprocessing)
ğ›â‚“ = Î£(ğ—, dims=2)/N # mean vector
ğ”¼Î¼Â² = Î£(ğ—.^2, dims=2)/N # vector of the second moment of ğ—
ÏƒÎ¼ = sqrt.(ğ”¼Î¼Â² - ğ›â‚“.^2) # vector of the standard deviation
ğ— = (ğ— .- ğ›â‚“)./ÏƒÎ¼ # zero mean and unit variance
ğ— = [fill(-1, size(ğ—,2))'; ğ—] # add the -1 input (bias)

ğ›d = Î£(ğƒ, dims=2)/N # mean vector
ğ”¼Î¼Â² = Î£(ğƒ.^2, dims=2)/N # vector of the second moment of ğƒ
ÏƒÎ¼ = sqrt.(ğ”¼Î¼Â² - ğ›d.^2) # vector of the standard deviation
ğƒ = (ğƒ .- ğ›d)./ÏƒÎ¼ # zero mean and unit variance
ğƒ ./= [maximum(abs.(i)) for i in eachrow(ğƒ)] # scale it to [-1, 1]

function shuffle_dataset(ğ—, ğƒ)
    shuffle_indices = Random.shuffle(1:size(ğ—,2))
    return ndims(ğƒ)==1 ? (ğ—[:, shuffle_indices], ğƒ[shuffle_indices]) : ğ—[:, shuffle_indices], ğƒ[:, shuffle_indices]
end

function train(ğ—, ğƒ, ğ”š, Ï†, Ï†Ê¼)
    L = length(ğ”š) # number of layers
    N = size(ğƒ, 2) # number of samples for the training dataset
    ğ„ = rand(size(ğƒ)...) # matrix with all errors
    for (n, (ğ±â‚â‚™â‚, ğâ‚â‚™â‚)) âˆˆ enumerate(zip(eachcol(ğ—), eachcol(ğƒ))) # n-th instance
        # initialize the output and the vetor of gradients of each layer!
        ğ”¶â‚â‚™â‚ = OrderedDict([(l, rand(size(ğ–â½Ë¡â¾â‚â‚™â‚, 1))) for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š])  # output of the l-th layer at the instant n
        ğ”¶Ê¼â‚â‚™â‚ = OrderedDict(ğ”¶â‚â‚™â‚) # diff of the output of the l-th layer at the instant n
        ğ”¡â‚â‚™â‚ = OrderedDict(ğ”¶â‚â‚™â‚) # all local gradients of all layers
        # forward phase!
        for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š # l-th layer
            ğ¯â½Ë¡â¾â‚â‚™â‚ = l==1 ? ğ–â½Ë¡â¾â‚â‚™â‚*ğ±â‚â‚™â‚ : ğ–â½Ë¡â¾â‚â‚™â‚*[-1; ğ”¶â‚â‚™â‚[l-1]] # induced local field
            ğ”¶â‚â‚™â‚[l] = map(Ï†, ğ¯â½Ë¡â¾â‚â‚™â‚)
            ğ”¶Ê¼â‚â‚™â‚[l] = map(Ï†Ê¼, ğ”¶â‚â‚™â‚[l])
        end
        # backward phase!
        for l âˆˆ L:-1:1
            if l==L # output layer
                ğâ‚â‚™â‚ = ğâ‚â‚™â‚ - ğ”¶â‚â‚™â‚[L]
                ğ„[:, n] = ğâ‚â‚™â‚
                ğ”¡â‚â‚™â‚[L] = ğ”¶Ê¼â‚â‚™â‚[L] âŠ™ ğâ‚â‚™â‚
            else # hidden layers
                ğ”¡â‚â‚™â‚[l] = ğ”¶Ê¼â‚â‚™â‚[l] âŠ™ ğ”š[l+1][:,2:end]'*ğ”¡â‚â‚™â‚[l+1] # vector of local gradients of the l-th layer
            end
            ğ”š[l] = l==1 ? ğ”š[l]+Î·*ğ”¡â‚â‚™â‚[l]*ğ±â‚â‚™â‚' : ğ”š[l]+Î·*ğ”¡â‚â‚™â‚[l]*[-1; ğ”¶â‚â‚™â‚[l-1]]' # learning equation
        end
    end

    RMSE = sqrt.(Î£(ğ„.^2, dims=2)./N)
    return ğ”š, RMSE # trained neural network synaptic weights and its RMSE
end

function test(ğ—, ğƒ, ğ”š, Ï†, is_output=false)
    L = length(ğ”š) # number of layers
    N = size(ğƒ, 2) # number of samples for the training dataset
    ğ„ = rand(size(ğƒ)...) # matrix with all errors
    ğ˜ = rand(size(ğƒ)...) # output signal
    for (n, (ğ±â‚â‚™â‚, ğâ‚â‚™â‚)) âˆˆ enumerate(zip(eachcol(ğ—), eachcol(ğƒ))) # n-th instance
        # initialize the output and the vetor of gradients of each layer!
        ğ”¶â‚â‚™â‚ = OrderedDict([(l, rand(size(ğ–â½Ë¡â¾â‚â‚™â‚, 1))) for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š])  # output of the l-th layer at the instant n
        # forward phase!
        for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š # l-th layer
            ğ¯â½Ë¡â¾â‚â‚™â‚ = l==1 ? ğ–â½Ë¡â¾â‚â‚™â‚*ğ±â‚â‚™â‚ : ğ–â½Ë¡â¾â‚â‚™â‚*[-1; ğ”¶â‚â‚™â‚[l-1]] # induced local field
            ğ”¶â‚â‚™â‚[l] = map(Ï†, ğ¯â½Ë¡â¾â‚â‚™â‚)
            l==L && (ğ„[:, n] = ğâ‚â‚™â‚ - ğ”¶â‚â‚™â‚[L]; ğ˜[:,n] = ğ”¶â‚â‚™â‚[L])
        end
    end
    if is_output
        return ğ˜
    else
        RMSE = sqrt.(Î£(ğ„.^2, dims=2)./N)
        return RMSE # RMSE of the test dataset
    end
end

## init
ğš³â‚œâ‚›â‚œ = rand(mâ‚‚, Náµ£) # vector of RMSE's for test dataset
for náµ£ âˆˆ 1:Náµ£
    println("Init realization $(náµ£)")
    # prepare the data!
    global ğ—, ğƒ = shuffle_dataset(ğ—, ğƒ)
    # hould-out
    ğ—â‚œáµ£â‚™ = ğ—[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    ğƒâ‚œáµ£â‚™ = ğƒ[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    ğ—â‚œâ‚›â‚œ = ğ—[:,size(ğƒâ‚œáµ£â‚™, 2)+1:end]
    ğƒâ‚œâ‚›â‚œ = ğƒ[:,size(ğƒâ‚œáµ£â‚™, 2)+1:end]
    # print("ğ—â‚œáµ£â‚™: $(size(ğ—â‚œáµ£â‚™))\nğ—â‚œâ‚›â‚œ: $(size(ğ—â‚œâ‚›â‚œ))")
    
    # grid search with k-fold cross validation! (that is not possible for a dataset with 1 million instances!)
    # (mâ‚, (Ï†, Ï†Ê¼, a)) = grid_search_cross_validation(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™, 10, (33:38, ((vâ‚â‚™â‚ -> 1/(1+â„¯^(-vâ‚â‚™â‚)), yâ‚â‚™â‚ -> yâ‚â‚™â‚*(1-yâ‚â‚™â‚), 1), (vâ‚â‚™â‚ -> (1-â„¯^(-vâ‚â‚™â‚))/(1+â„¯^(-vâ‚â‚™â‚)), yâ‚â‚™â‚ -> .5(1-yâ‚â‚™â‚^2), 2))))
    (mâ‚, (Ï†, Ï†Ê¼, a)) = (7, (vâ‚â‚™â‚ -> (1-â„¯^(-vâ‚â‚™â‚))/(1+â„¯^(-vâ‚â‚™â‚)), yâ‚â‚™â‚ -> .5(1-yâ‚â‚™â‚^2), 1))
    # println("For the realization $(náµ£)")
    # println("best mâ‚: $(mâ‚)")
    # println("best Ï†: $(a==1 ? "logistic" : "Hyperbolic")")
    
    # initialize!
    ğ”š = OrderedDict(1 => rand(mâ‚, Nâ‚+1), 2 => rand(mâ‚‚, mâ‚+1)) # 1 => first (hidden) layer 2 => second (output) layer 
    ğš³â‚œáµ£â‚™ = rand(mâ‚‚, Nâ‚‘) # vector of accuracies for train dataset (to see its evolution during training phase)
    
    # train!
    for nâ‚‘ âˆˆ 1:Nâ‚‘ # for each epoch
        ğ”š, ğš³â‚œáµ£â‚™[:, nâ‚‘] = train(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™, ğ”š, Ï†, Ï†Ê¼)
        ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™ = shuffle_dataset(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™)
    end
    # test!
    global ğš³â‚œâ‚›â‚œ[:, náµ£] = test(ğ—â‚œâ‚›â‚œ, ğƒâ‚œâ‚›â‚œ, ğ”š, Ï†) # accuracy for this realization
    
    # plot training dataset accuracy evolution
    local fig = plot(ğš³â‚œáµ£â‚™', ylims=(0,2), xlabel="Epochs", ylabel="Accuracy", linewidth=2)
    savefig(fig, "figs/electric-motor-temp - training dataset RMSE evolution for realization $(náµ£).png")

    # make the neural network weights of the first realization global
    náµ£==1 && global ğ”š, Ï†
end

# analyze the accuracy statistics of each independent realization
ğ”¼ğš³â‚œâ‚›â‚œ = Î£(ğš³â‚œâ‚›â‚œ, dims=2)./Náµ£ # mean vector of torque and speed motor
ğ”¼ğš³â‚œâ‚›â‚œÂ² = Î£(ğš³â‚œâ‚›â‚œ.^2)./Náµ£ # second moment of both features
Ïƒğš³â‚œâ‚›â‚œ = sqrt.(ğ”¼ğš³â‚œâ‚›â‚œÂ² .- ğ”¼ğš³â‚œâ‚›â‚œ.^2) # standard deviation vector of torque and speed motor

for (i, label) âˆˆ enumerate(("coolant", "stator winding", "stator tooth", "permanent magnet", "stator yoke"))
    println("* Mean RMSE for the $(label): $(ğ”¼ğš³â‚œâ‚›â‚œ[i])")
    println("* Standard deviation for the $(label): $(Ïƒğš³â‚œâ‚›â‚œ[i])\n")
end

## make plot!
ğƒ = [df.coolant df.stator_winding df.stator_tooth df.pm df.stator_yoke]'
ğ— = [df.torque df.motor_speed df.u_q df.u_d df.i_q df.i_d df.ambient]'

## Standardize dataset (Preprocessing)
ğ›â‚“ = Î£(ğ—, dims=2)/N # mean vector
ğ”¼Î¼Â² = Î£(ğ—.^2, dims=2)/N # vector of the second moment of ğ—
ÏƒÎ¼ = sqrt.(ğ”¼Î¼Â² - ğ›â‚“.^2) # vector of the standard deviation
ğ— = (ğ— .- ğ›â‚“)./ÏƒÎ¼ # zero mean and unit variance
ğ— = [fill(-1, size(ğ—,2))'; ğ—] # add the -1 input (bias)

ğ›d = Î£(ğƒ, dims=2)/N # mean vector
ğ”¼Î¼Â² = Î£(ğƒ.^2, dims=2)/N # vector of the second moment of ğƒ
ÏƒÎ¼ = sqrt.(ğ”¼Î¼Â² - ğ›d.^2) # vector of the standard deviation
ğƒ = (ğƒ .- ğ›d)./ÏƒÎ¼ # zero mean and unit variance
ğƒ ./= [maximum(abs.(i)) for i in eachrow(ğƒ)] # scale it to [-1, 1]

ğ˜ = test(ğ—, ğƒ, ğ”š, Ï†, true) # accuracy for this realization

fig = plot([ğ˜[2,:] ğƒ[2,:]], title="Stator winding estimation", label=["Estimated signal" "Desired signal"], linestyle=[:dashdot :solid], linewidth=3, xlabel="Samples", ylabel="Stator winding temperature (in Â°C)")

savefig(fig, "figs/electric-motor-temp - Stator winding regression.png")