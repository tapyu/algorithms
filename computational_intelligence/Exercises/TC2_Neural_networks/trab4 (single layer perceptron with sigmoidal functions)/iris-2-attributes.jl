using FileIO, JLD2, Random, LinearAlgebra, Plots, LaTeXStrings
Î£=sum

## algorithm parameters hyperparameters
Náµ£ = 20 # number of realizations
Nâ‚ = 2 # number of Attributes, that is, input vector size at each instance. They are petal length, petal width
N = 150 # number of instances(samples)
Nâ‚œáµ£â‚™ = 80 # % percentage of instances for the train dataset
Nâ‚œâ‚›â‚œ = 20 # % percentage of instances for the test dataset
c = 3 # number of perceptrons (neurons) of the single layer
Nâ‚‘ = 100 # number of epochs
Î± = 0.01 # learning step

## load the data
ğ—, labels = FileIO.load("Datasets/Iris [uci]/iris.jld2", "ğ—", "ğ") # ğ— â¡ [attributes X instances]
ğ— = [fill(-1, size(ğ—,2))'; ğ—[3:4,:]] # add the -1 input (bias)
ğƒ = rand(c,0)
for label âˆˆ labels
    global ğƒ = [ğƒ one_hot_encoding(label)]
end

## useful functions
function shuffle_dataset(ğ—, ğƒ)
    shuffle_indices = Random.shuffle(1:size(ğ—,2))
    return ğ—[:, shuffle_indices], ğƒ[:, shuffle_indices]
end

function train(ğ—, ğƒ, ğ–â‚â‚™â‚, is_training_accuracy=true)
    Ï† = uâ‚â‚™â‚ -> 1/(1+â„¯^(-uâ‚â‚™â‚)) # logistic function
    Ï†Ê¼ = yâ‚â‚™â‚ -> yâ‚â‚™â‚*(1-yâ‚â‚™â‚) # where yâ‚â‚™â‚=Ï†(uâ‚â‚™â‚)
    Nâ‚‘ = 0 # number of errors â¡ misclassifications
    for (ğ±â‚â‚™â‚, ğâ‚â‚™â‚) âˆˆ zip(eachcol(ğ—), eachcol(ğƒ))
        ğ›â‚â‚™â‚ = ğ–â‚â‚™â‚*ğ±â‚â‚™â‚ # induced local field
        ğ²â‚â‚™â‚ = map(Ï†, ğ›â‚â‚™â‚)
        ğâ‚â‚™â‚ = ğâ‚â‚™â‚ - ğ²â‚â‚™â‚
        ğ²Ê¼â‚â‚™â‚ = map(Ï†Ê¼, ğ²â‚â‚™â‚)
        ğ›…â‚â‚™â‚ = ğâ‚â‚™â‚ .* ğ²Ê¼â‚â‚™â‚ # vector of local gradients
        ğ–â‚â‚™â‚ += Î±*ğ›…â‚â‚™â‚*ğ±â‚â‚™â‚' # learning equation (Julia performs broadcasting here)
        # this part is optional: only if it is interested in seeing the accuracy evolution of the training dataset throughout the epochs
        i = findfirst(x->x==maximum(ğ²â‚â‚™â‚), ğ²â‚â‚™â‚) # predicted value â†’ choose the highest activation function output as the selected class
        Nâ‚‘ = ğâ‚â‚™â‚[i]==1 ? Nâ‚‘ : Nâ‚‘+1
    end
    if is_training_accuracy
        accuracy = (size(ğƒ,2)-Nâ‚‘)/size(ğƒ,2)
        return ğ–â‚â‚™â‚, accuracy
    else
        return  ğ–â‚â‚™â‚
    end
end

function test(ğ—, ğƒ, ğ–â‚â‚™â‚, get_predictions=false)
    Ï† = uâ‚â‚™â‚ -> 1/(1+â„¯^(-uâ‚â‚™â‚)) # logistic function
    Nâ‚‘ = 0 # number of errors â¡ misclassifications
    ğ² = Vector{Int64}(undef, size(ğ—, 2)) # vector of all predictions
    for (n, (ğ±â‚â‚™â‚, ğâ‚â‚™â‚)) âˆˆ enumerate(zip(eachcol(ğ—), eachcol(ğƒ)))
        ğ›â‚â‚™â‚ = ğ–â‚â‚™â‚*ğ±â‚â‚™â‚ # induced local field
        ğ²â‚â‚™â‚ = map(Ï†, ğ›â‚â‚™â‚) # perceptron output (a vector) at instant n
        ğ²[n] = findfirst(x->x==maximum(ğ²â‚â‚™â‚), ğ²â‚â‚™â‚) # predicted value â†’ choose the highest activation function output as the selected class
        Nâ‚‘ = ğâ‚â‚™â‚[ğ²[n]]==1 ? Nâ‚‘ : Nâ‚‘+1
    end
    accuracy = (size(ğƒ,2)-Nâ‚‘)/size(ğƒ,2)
    if get_predictions
        return accuracy, ğ²
    else
        return accuracy
    end
end

function one_hot_encoding(label)
    return ["setosa", "virginica", "versicolor"].==label
end

## init
for náµ£ âˆˆ 1:Náµ£
    # initialize!
    ğ–â‚â‚™â‚ = rand(c, Nâ‚+1) # [ğ°â‚áµ€; ğ°â‚‚áµ€; ...; ğ°áµ€_c]

    # prepare the data!
    global ğ—, ğƒ = shuffle_dataset(ğ—, ğƒ)
    # hould-out
    global ğ—â‚œáµ£â‚™ = ğ—[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    global ğƒâ‚œáµ£â‚™ = ğƒ[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    global ğ—â‚œâ‚›â‚œ = ğ—[:,size(ğƒâ‚œáµ£â‚™, 2)+1:end]
    global ğƒâ‚œâ‚›â‚œ = ğƒ[:,size(ğƒâ‚œáµ£â‚™, 2)+1:end]

    # train!
    for nâ‚‘ âˆˆ 1:Nâ‚‘ # for each epoch
        ğ–â‚â‚™â‚, _ = train(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™, ğ–â‚â‚™â‚)
        ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™ = shuffle_dataset(ğ—â‚œáµ£â‚™, ğƒâ‚œáµ£â‚™)
    end

    # decision surface (for the 1th realization only)
    if náµ£ == 1
        ## predictors of each class (basically it is what is done on test(), but only with the attributes as inputs)
        #setosa
        y_setosa = function predict_setosa(xâ‚ƒ, xâ‚„)
            Ï† = uâ‚â‚™â‚ -> 1/(1+â„¯^(-uâ‚â‚™â‚)) # logistic function
            ğ±â‚â‚™â‚ = [-1, xâ‚ƒ, xâ‚„]
            ğ›â‚â‚™â‚ = ğ–â‚â‚™â‚*ğ±â‚â‚™â‚ # induced local field
            ğ²â‚â‚™â‚ = map(Ï†, ğ›â‚â‚™â‚) # perceptron output (a vector) at instant n
            if ğ²â‚â‚™â‚[1] == maximum(ğ²â‚â‚™â‚)
                return 1
            else
                return 0
            end
        end
        # virginica
        y_virginica = function predict_virginica(xâ‚ƒ, xâ‚„)
            Ï† = uâ‚â‚™â‚ -> 1/(1+â„¯^(-uâ‚â‚™â‚)) # logistic function
            ğ±â‚â‚™â‚ = [-1, xâ‚ƒ, xâ‚„]
            ğ›â‚â‚™â‚ = ğ–â‚â‚™â‚*ğ±â‚â‚™â‚ # induced local field
            ğ²â‚â‚™â‚ = map(Ï†, ğ›â‚â‚™â‚) # perceptron output (a vector) at instant n
            if ğ²â‚â‚™â‚[2] == maximum(ğ²â‚â‚™â‚)
                return 1
            else
                return 0
            end
        end
        # versicolor
        y_versicolor = function predict_versicolor(xâ‚ƒ, xâ‚„)
            Ï† = uâ‚â‚™â‚ -> 1/(1+â„¯^(-uâ‚â‚™â‚)) # logistic function
            ğ±â‚â‚™â‚ = [-1, xâ‚ƒ, xâ‚„]
            ğ›â‚â‚™â‚ = ğ–â‚â‚™â‚*ğ±â‚â‚™â‚ # induced local field
            ğ²â‚â‚™â‚ = map(Ï†, ğ›â‚â‚™â‚) # perceptron output (a vector) at instant n
            if ğ²â‚â‚™â‚[3] == maximum(ğ²â‚â‚™â‚)
                return 1
            else
                return 0
            end
        end

        # plot the surface for each class
        xâ‚ƒ_range = floor(minimum(ğ—[2,:])):.1:ceil(maximum(ğ—[2,:]))
        xâ‚„_range = floor(minimum(ğ—[3,:])):.1:ceil(maximum(ğ—[3,:]))
        for (i, (y, desired_label, camera_pos)) âˆˆ enumerate(zip((y_setosa, y_virginica, y_versicolor), ("setosa", "virginica", "versicolor"), ((60,40,0), (60,40,0), (160,40,0))))
            p = surface(xâ‚ƒ_range, xâ‚„_range, y, camera=camera_pos, xlabel = "petal length", ylabel = "petal width", zlabel="decision surface")
    
            # scatter plot for the petal length and petal length width for the setosa class
            # train and desired label 
            scatter!(ğ—â‚œáµ£â‚™[2,ğƒâ‚œáµ£â‚™[i,:].==1], ğ—â‚œáµ£â‚™[3,ğƒâ‚œáµ£â‚™[i,:].==1], ones(length(filter(x->x==1, ğƒâ‚œáµ£â‚™[i,:]))), markershape = :hexagon, markersize = 4, markeralpha = 0.6, markercolor = :green, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, xlabel = "petal\nlength", ylabel = "petal width", label = "$(desired_label) train set")
            
            # test and desired label 
            scatter!(ğ—â‚œâ‚›â‚œ[2,ğƒâ‚œâ‚›â‚œ[i,:].==1], ğ—â‚œâ‚›â‚œ[3,ğƒâ‚œâ‚›â‚œ[i,:].==1], ones(length(filter(x->x==1, ğƒâ‚œâ‚›â‚œ[i,:]))), markershape = :cross, markersize = 4, markeralpha = 0.6, markercolor = :green, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, xlabel = "petal\nlength", ylabel = "petal width", label = "$(desired_label) test set")
    
            # train and not desired label 
            scatter!(ğ—â‚œáµ£â‚™[2,ğƒâ‚œáµ£â‚™[i,:].==0], ğ—â‚œáµ£â‚™[3,ğƒâ‚œáµ£â‚™[i,:].==0], zeros(length(filter(x->x==0, ğƒâ‚œáµ£â‚™[i,:]))), markershape = :hexagon, markersize = 4, markeralpha = 0.6, markercolor = :red, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, xlabel = "petal\nlength", ylabel = "petal width", label = "not $(desired_label) train set")
    
            # test and not desired label 
            scatter!(ğ—â‚œâ‚›â‚œ[2,ğƒâ‚œâ‚›â‚œ[i,:].==0], ğ—â‚œâ‚›â‚œ[3,ğƒâ‚œâ‚›â‚œ[i,:].==0], zeros(length(filter(x->x==0, ğƒâ‚œâ‚›â‚œ[i,:]))), markershape = :cross, markersize = 4, markeralpha = 0.6, markercolor = :red, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, xlabel = "petal\nlength", ylabel = "petal width", label = "not $(desired_label) test set")
            
            title!("Decision surface for the class $(desired_label)")
            display(p)
            savefig(p,"trab4 (single layer perceptron with sigmoidal functions)/figs/decision-surface-for-$(desired_label).png")
        end

    end
end