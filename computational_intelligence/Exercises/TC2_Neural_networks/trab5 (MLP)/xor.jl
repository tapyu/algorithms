using FileIO, JLD2, Random, LinearAlgebra, Plots, LaTeXStrings, DataStructures
include("grid_search_cross_validation.jl")
Î£=sum
âŠ™ = .* # Hadamard product

function one_hot_encoding(label)
    return ["setosa", "virginica", "versicolor"].==label
end

function shuffle_dataset(ğ—, ğ)
    shuffle_indices = Random.shuffle(1:size(ğ—,2))
    return ğ—[:, shuffle_indices], ğ[shuffle_indices]
end

function train(ğ—, ğ, ğ”š, Ï†, Ï†Ê¼)
    L = length(ğ”š) # number of layers
    Nâ‚‘ = 0 # number of errors â¡ misclassifications
    for (ğ±â‚â‚™â‚, dâ‚â‚™â‚) âˆˆ zip(eachcol(ğ—), ğ) # n-th instance
        # initialize the output and the vetor of gradients of each layer!
        ğ”¶â‚â‚™â‚ = OrderedDict([(l, rand(size(ğ–â½Ë¡â¾â‚â‚™â‚, 1))) for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š])  # output of the l-th layer at the instant n
        ğ”¶Ê¼â‚â‚™â‚ = OrderedDict(ğ”¶â‚â‚™â‚) # diff of the output of the l-th layer at the instant n
        ğ”¡â‚â‚™â‚ = OrderedDict(ğ”¶â‚â‚™â‚) # all local gradients of all layers
        # forward phase!
        for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š # l-th layer
            ğ¯â½Ë¡â¾â‚â‚™â‚ = l==1 ? ğ–â½Ë¡â¾â‚â‚™â‚*ğ±â‚â‚™â‚ : ğ–â½Ë¡â¾â‚â‚™â‚*[-1; ğ”¶â‚â‚™â‚[l-1]] # induced local field
            ğ”¶â‚â‚™â‚[l] = map(Ï†, ğ¯â½Ë¡â¾â‚â‚™â‚)
            ğ”¶Ê¼â‚â‚™â‚[l] = map(Ï†Ê¼, ğ”¶â‚â‚™â‚[l])
        end
        # backward phase!
        for l âˆˆ L:-1:1
            if l==L # output layer
                eâ‚â‚™â‚ = [dâ‚â‚™â‚] - ğ”¶â‚â‚™â‚[L]
                yâ‚â‚™â‚ = ğ”¶â‚â‚™â‚[L][1]>.5 ? 1 : 0 # predicted value â†’ get the result of the step function
                Nâ‚‘ = dâ‚â‚™â‚==yâ‚â‚™â‚ ? Nâ‚‘ : Nâ‚‘+1 # count error if it occurs
                ğ”¡â‚â‚™â‚[L] = ğ”¶Ê¼â‚â‚™â‚[L] âŠ™ eâ‚â‚™â‚
            else # hidden layers
                ğ”¡â‚â‚™â‚[l] = ğ”¶Ê¼â‚â‚™â‚[l] âŠ™ ğ”š[l+1][:,2:end]'*ğ”¡â‚â‚™â‚[l+1] # vector of local gradients of the l-th layer
            end
            ğ”š[l] = l==1 ? ğ”š[l]+Î·*ğ”¡â‚â‚™â‚[l]*ğ±â‚â‚™â‚' : ğ”š[l]+Î·*ğ”¡â‚â‚™â‚[l]*[-1; ğ”¶â‚â‚™â‚[l-1]]' # learning equation
        end
    end
    return ğ”š, (length(ğ)-Nâ‚‘)/length(ğ) # trained neural network synaptic weights and its accuracy
end

function test(ğ—, ğ, ğ”š, Ï†, is_confusion_matrix=false)
    L = length(ğ”š) # number of layers
    Nâ‚‘ = 0 # number of errors â¡ misclassification
    ğ² = rand(length(ğ))
    for (n, (ğ±â‚â‚™â‚, dâ‚â‚™â‚)) âˆˆ enumerate(zip(eachcol(ğ—), ğ)) # n-th instance
        # initialize the output and the vetor of gradients of each layer!
        ğ”¶â‚â‚™â‚ = OrderedDict([(l, rand(size(ğ–â½Ë¡â¾â‚â‚™â‚, 1))) for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š])  # output of the l-th layer at the instant n
        # forward phase!
        for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š # l-th layer
            ğ¯â½Ë¡â¾â‚â‚™â‚ = l==1 ? ğ–â½Ë¡â¾â‚â‚™â‚*ğ±â‚â‚™â‚ : ğ–â½Ë¡â¾â‚â‚™â‚*[-1; ğ”¶â‚â‚™â‚[l-1]] # induced local field
            ğ”¶â‚â‚™â‚[l] = map(Ï†, ğ¯â½Ë¡â¾â‚â‚™â‚)
            if l==L # output layer
                ğ²[n] = ğ”¶â‚â‚™â‚[L][1]>.5 ? 1 : 0 # predicted value â†’ get the result of the step function
                Nâ‚‘ = dâ‚â‚™â‚==ğ²[n] ? Nâ‚‘ : Nâ‚‘+1 # count error if it occurs
            end
        end
    end
    
    if is_confusion_matrix
        return Int.(ğ²)
    else
        return (length(ğ)-Nâ‚‘)/length(ğ)
    end
end

## algorithm parameters and hyperparameters
K = 2 # number of classes (1 and 0)
N = 200 # number of instances
Nâ‚œáµ£â‚™ = 80 # % percentage of instances for the train dataset
Nâ‚œâ‚›â‚œ = 20 # % percentage of instances for the test dataset
Nâ‚ = 2 # number of number of attributes (xâ‚ and xâ‚‚)
Náµ£ = 20 # number of realizations
Nâ‚‘ = 100 # number of epochs
mâ‚‚ = 1 # number of perceptrons (neurons) of the output layer (only one since it is enough to classify 0 or 1)
Î· = 2 # learning step

ğ— = [fill(0, 100)' fill(1, 100)'; fill(1, 50)' fill(0, 100)' fill(1, 50)']
ğ = map(x -> x[1] âŠ» x[2], eachcol(ğ—))
## Standardize dataset (Preprocessing)
ğ›â‚“ = Î£(ğ—, dims=2)/N # mean vector
ğ”¼Î¼Â² = Î£(ğ—.^2, dims=2)/N # vector of the second moment of ğ—
ÏƒÎ¼ = sqrt.(ğ”¼Î¼Â² - ğ›â‚“.^2) # vector of the standard deviation
ğ— = (ğ— .- ğ›â‚“)./ÏƒÎ¼ # zero mean and unit variance
ğ— = [fill(-1, size(ğ—,2))'; ğ—] # add the -1 input (bias)

## init
ğ›â‚œâ‚›â‚œ = fill(NaN, Náµ£) # vector of accuracies for test dataset
for náµ£ âˆˆ 1:Náµ£
    # prepare the data!
    global ğ—, ğ = shuffle_dataset(ğ—, ğ)
    # hould-out
    ğ—â‚œáµ£â‚™ = ğ—[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
    ğâ‚œáµ£â‚™ = ğ[1:(N*Nâ‚œáµ£â‚™)Ã·100]
    ğ—â‚œâ‚›â‚œ = ğ—[:,length(ğâ‚œáµ£â‚™)+1:end]
    ğâ‚œâ‚›â‚œ = ğ[length(ğâ‚œáµ£â‚™)+1:end]

    # grid search with k-fold cross validation!
    (mâ‚, (Ï†, Ï†Ê¼, a)) = grid_search_cross_validation(ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™, 10, (1:3, ((vâ‚â‚™â‚ -> 1/(1+â„¯^(-vâ‚â‚™â‚)), yâ‚â‚™â‚ -> yâ‚â‚™â‚*(1-yâ‚â‚™â‚), 1), (vâ‚â‚™â‚ -> (1-â„¯^(-vâ‚â‚™â‚))/(1+â„¯^(-vâ‚â‚™â‚)), yâ‚â‚™â‚ -> .5(1-yâ‚â‚™â‚^2), 2), (vâ‚â‚™â‚ -> vâ‚â‚™â‚>0 ? 1 : 0, yâ‚â‚™â‚ -> 1, 3))))
    println("For the realization $(náµ£)")
    println("best mâ‚: $(mâ‚)")
    println("best Ï†: $(a==1 ? "logistic" : (a==2 ? "Hyperbolic" : "Mcculloch and pitts"))")
    
    # initialize!
    ğ”š = OrderedDict(1 => rand(mâ‚, Nâ‚+1), 2 => rand(mâ‚‚, mâ‚+1)) # 1 => first layer (hidden layer) 2 => second layer 
    ğ›â‚œáµ£â‚™ = fill(NaN, Nâ‚‘) # vector of accuracies for train dataset (to see its evolution during training phase)

    # train!
    for nâ‚‘ âˆˆ 1:Nâ‚‘ # for each epoch
        ğ”š, ğ›â‚œáµ£â‚™[nâ‚‘] = train(ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™, ğ”š, Ï†, Ï†Ê¼)
        ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™ = shuffle_dataset(ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™)
    end
    # test!
    global ğ›â‚œâ‚›â‚œ[náµ£] = test(ğ—â‚œâ‚›â‚œ, ğâ‚œâ‚›â‚œ, ğ”š, Ï†) # accuracy for this realization
    
    # plot training dataset accuracy evolution
    local fig = plot(ğ›â‚œáµ£â‚™, xlabel="Epochs", ylabel="Accuracy", linewidth=2)
    # savefig(fig, "trab5 (MLP)/figs/xor - training dataset accuracy evolution for realization $(náµ£)- Î¼$(ğ›â‚œâ‚›â‚œ[náµ£]).png")
    
    ## predictor of the class (basically it is what is done on test(), but only with the attributes as inputs)
    y = function predict(xâ‚, xâ‚‚)
        ğ”¶â‚â‚™â‚ = OrderedDict([(l, rand(size(ğ–â½Ë¡â¾â‚â‚™â‚, 1))) for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š])
        L = length(ğ”š)
        ğ±â‚â‚™â‚ = [-1, xâ‚, xâ‚‚]
        for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š # l-th layer
            ğ¯â½Ë¡â¾â‚â‚™â‚ = l==1 ? ğ–â½Ë¡â¾â‚â‚™â‚*ğ±â‚â‚™â‚ : ğ–â½Ë¡â¾â‚â‚™â‚*[-1; ğ”¶â‚â‚™â‚[l-1]] # induced local field
            ğ”¶â‚â‚™â‚[l] = map(Ï†, ğ¯â½Ë¡â¾â‚â‚™â‚)
            if l==L # output layer
                return ğ”¶â‚â‚™â‚[L][1]>.5 ? 1 : 0
            end
        end
    end

    y1 = function predict_hidden1(xâ‚, xâ‚‚) # the heatmap of the first hidden layer
        yâ½Â¹â¾â‚â‚â‚™â‚ = rand(size(ğ”š[1], 1))
        ğ±â‚â‚™â‚ = [-1, xâ‚, xâ‚‚]

        ğ¯â½Ë¡â¾â‚â‚™â‚ = ğ”š[1]*ğ±â‚â‚™â‚# induced local field
        yâ½Â¹â¾â‚â‚â‚™â‚ = map(Ï†, ğ¯â½Ë¡â¾â‚â‚™â‚)
        return yâ½Â¹â¾â‚â‚â‚™â‚[1]>.5 ? 1 : 0
    end

    y2 = function predict_hidden2(xâ‚, xâ‚‚) # the heatmap of the first hidden layer
        yâ½Â¹â¾â‚â‚â‚™â‚ = rand(size(ğ”š[1], 1))
        ğ±â‚â‚™â‚ = [-1, xâ‚, xâ‚‚]

        ğ¯â½Ë¡â¾â‚â‚™â‚ = ğ”š[1]*ğ±â‚â‚™â‚# induced local field
        yâ½Â¹â¾â‚â‚â‚™â‚ = map(Ï†, ğ¯â½Ë¡â¾â‚â‚™â‚)
        return yâ½Â¹â¾â‚â‚â‚™â‚[2]>.5 ? 1 : 0
    end
    
    # plot heatmap for the 1th realization
    xâ‚_range = -1:.1:2
    xâ‚‚_range = -1:.1:2
    
    fig = contour(xâ‚_range, xâ‚‚_range, y, xlabel = L"x_1", ylabel = L"x_2", fill=true, levels=1)
    
    # train 0 label
    scatter!(ğ—â‚œáµ£â‚™[2, ğâ‚œáµ£â‚™.==0], ğ—â‚œáµ£â‚™[3, ğâ‚œáµ£â‚™.==0], markershape = :hexagon, markersize = 8, markeralpha = 0.6, markercolor = :green, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "0 label [train]")
    
    # test 0 label
    scatter!(ğ—â‚œâ‚›â‚œ[2, ğâ‚œâ‚›â‚œ.==0], ğ—â‚œâ‚›â‚œ[3, ğâ‚œâ‚›â‚œ.==0], markershape = :dtriangle, markersize = 8, markeralpha = 0.6, markercolor = :green, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "0 label [test]")
    
    # train 1 label
    scatter!(ğ—â‚œáµ£â‚™[2, ğâ‚œáµ£â‚™.==1], ğ—â‚œáµ£â‚™[3, ğâ‚œáµ£â‚™.==1], markershape = :hexagon, markersize = 8, markeralpha = 0.6, markercolor = :white, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "1 label [train]")
        
    # test 1 label
    scatter!(ğ—â‚œâ‚›â‚œ[2, ğâ‚œâ‚›â‚œ.==1], ğ—â‚œâ‚›â‚œ[3, ğâ‚œâ‚›â‚œ.==1], markershape = :dtriangle, markersize = 8, markeralpha = 0.6, markercolor = :white, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black, label = "1 label [test]")
    
    title!("Heatmap")
    savefig(fig, "trab5 (MLP)/figs/XOR problem - heatmap - nr$(náµ£).png") #  - Î¼$(ğ›â‚œâ‚›â‚œ[náµ£])
    
    # heatmap for the hidden neuron 1!
    fig = contour(xâ‚_range, xâ‚‚_range, y1, xlabel=L"x_1", ylabel=L"x_2", fill=true, levels=1, title="Heatmap of the first hidden neuron")
    scatter!([1 1 0 0], [1 0 1 0], markershape = :hexagon, markersize = 8, markeralpha = 0.6, markercolor = :white, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black)
    # savefig(fig, "trab5 (MLP)/figs/XOR problem - heatmap - hidden neuron 1 - nr$(náµ£) - Î¼$(ğ›â‚œâ‚›â‚œ[náµ£]).png")

    # heatmap for the hidden neuron 2!
    # fig = contour(xâ‚_range, xâ‚‚_range, y2, xlabel=L"x_1", ylabel=L"x_2", fill=true, levels=1, title="Heatmap of the second hidden neuron")
    scatter!([1 1 0 0], [1 0 1 0], markershape = :hexagon, markersize = 8, markeralpha = 0.6, markercolor = :white, markerstrokewidth = 3, markerstrokealpha = 0.2, markerstrokecolor = :black)
    # savefig(fig, "trab5 (MLP)/figs/XOR problem - heatmap - hidden neuron 2 - nr$(náµ£) - Î¼$(ğ›â‚œâ‚›â‚œ[náµ£]).png")
    # if ğ›â‚œâ‚›â‚œ[náµ£] != 1 # make heatmap plot!
    # end
    
    # confusion matrix
    if ğ›â‚œâ‚›â‚œ[náµ£] == 1 && !isfile("trab5 (MLP)/figs/xor-confusion-matrix.png")
        ğ‚ = zeros(2,2)
        ğ²â‚œâ‚›â‚œ = test(ğ—â‚œâ‚›â‚œ, ğâ‚œâ‚›â‚œ, ğ”š, Ï†, true)
        for n âˆˆ 1:length(ğ²â‚œâ‚›â‚œ)
            # predicted x true label
            ğ‚[ğ²â‚œâ‚›â‚œ[n]+1, ğâ‚œâ‚›â‚œ[n]+1] += 1
        end
        h = heatmap(ğ‚, xlabel="Predicted labels", ylabel="True labels", xticks=(1:2, (0, 1)), yticks=(1:2, (0, 1)), title="Confusion matrix")
        savefig(h, "trab5 (MLP)/figs/xor-confusion-matrix.png") # TODO: put the number onto each confusion square
    end
end

# analyze the accuracy statistics of each independent realization
Î¼Ì„â‚œâ‚›â‚œ = Î£(ğ›â‚œâ‚›â‚œ)/Náµ£ # mean
ğ”¼Î¼Â² = Î£(ğ›â‚œâ‚›â‚œ.^2)/Náµ£
ÏƒÎ¼ = sqrt(ğ”¼Î¼Â² - Î¼Ì„â‚œâ‚›â‚œ^2) # standard deviation

println("Mean: $(Î¼Ì„â‚œâ‚›â‚œ)")
println("Standard deviation: $(ÏƒÎ¼)")

# plot(ğ›â‚œâ‚›â‚œ)