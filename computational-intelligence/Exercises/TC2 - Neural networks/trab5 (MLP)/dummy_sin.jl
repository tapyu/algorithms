using FileIO, JLD2, Random, LinearAlgebra, Plots, LaTeXStrings, DataStructures
include("grid_search_cross_validation.jl")
Î£=sum
âŠ™ = .* # Hadamard product

# generate dataset
N = 500 # number of instances
ğ± = range(0, 2Ï€, length=N)
ğ = map(xâ‚â‚™â‚ -> 3sin(xâ‚â‚™â‚)+1, ğ±)

function shuffle_dataset(ğ±, ğ)
    shuffle_indices = Random.shuffle(1:length(ğ±))
    return ğ±[shuffle_indices], ğ[shuffle_indices]
end

function train(ğ±, ğ, ğ”š, Ï†, Ï†Ê¼)
    L = length(ğ”š) # number of layers
    N = length(ğ) # number of samples for the training dataset
    ğ = rand(length(ğ)) # matrix with all errors
    for (n, (xâ‚â‚™â‚, dâ‚â‚™â‚)) âˆˆ enumerate(zip(ğ±, ğ)) # n-th instance
        # initialize the output and the vetor of gradients of each layer!
        ğ”¶â‚â‚™â‚ = OrderedDict([(l, rand(size(ğ–â½Ë¡â¾â‚â‚™â‚, 1))) for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š])  # output of the l-th layer at the instant n
        ğ”¶Ê¼â‚â‚™â‚ = OrderedDict(ğ”¶â‚â‚™â‚) # diff of the output of the l-th layer at the instant n
        ğ”¡â‚â‚™â‚ = OrderedDict(ğ”¶â‚â‚™â‚) # all local gradients of all layers
        # forward phase!
        for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š # l-th layer
            ğ¯â½Ë¡â¾â‚â‚™â‚ = l==1 ? ğ–â½Ë¡â¾â‚â‚™â‚*[-1; xâ‚â‚™â‚] : ğ–â½Ë¡â¾â‚â‚™â‚*[-1; ğ”¶â‚â‚™â‚[l-1]] # induced local field
            ğ”¶â‚â‚™â‚[l] = map(Ï†, ğ¯â½Ë¡â¾â‚â‚™â‚)
            ğ”¶Ê¼â‚â‚™â‚[l] = map(Ï†Ê¼, ğ”¶â‚â‚™â‚[l])
        end
        # backward phase!
        for l âˆˆ L:-1:1
            if l==L # output layer
                eâ‚â‚™â‚ = dâ‚â‚™â‚ - ğ”¶â‚â‚™â‚[L][1]
                ğ[n] = eâ‚â‚™â‚
                ğ”¡â‚â‚™â‚[L] = ğ”¶Ê¼â‚â‚™â‚[L] âŠ™ eâ‚â‚™â‚
            else # hidden layers
                ğ”¡â‚â‚™â‚[l] = ğ”¶Ê¼â‚â‚™â‚[l] âŠ™ ğ”š[l+1][:,2:end]'*ğ”¡â‚â‚™â‚[l+1] # vector of local gradients of the l-th layer
            end
            ğ”š[l] = l==1 ? ğ”š[l]+Î·*ğ”¡â‚â‚™â‚[l]*[-1; xâ‚â‚™â‚]' : ğ”š[l]+Î·*ğ”¡â‚â‚™â‚[l]*[-1; ğ”¶â‚â‚™â‚[l-1]]' # learning equation
        end
    end
    RMSE = âˆš(Î£(ğ.^2)/N)
    return ğ”š, RMSE # trained neural network synaptic weights and its RMSE
end

function test(ğ±, ğ, ğ”š, Ï†, is_output=false)
    L = length(ğ”š) # number of layers
    N = length(ğ) # number of samples for the training dataset
    ğ = rand(length(ğ)...) # matrix with all errors
    ğ² = rand(length(ğ)...)
    for (n, (xâ‚â‚™â‚, eâ‚â‚™â‚)) âˆˆ enumerate(zip(ğ±, ğ)) # n-th instance
        # initialize the output and the vetor of gradients of each layer!
        ğ”¶â‚â‚™â‚ = OrderedDict([(l, rand(size(ğ–â½Ë¡â¾â‚â‚™â‚, 1))) for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š])  # output of the l-th layer at the instant n
        # forward phase!
        for (l, ğ–â½Ë¡â¾â‚â‚™â‚) âˆˆ ğ”š # l-th layer
            ğ¯â½Ë¡â¾â‚â‚™â‚ = l==1 ? ğ–â½Ë¡â¾â‚â‚™â‚*[-1; xâ‚â‚™â‚] : ğ–â½Ë¡â¾â‚â‚™â‚*[-1; ğ”¶â‚â‚™â‚[l-1]] # induced local field
            ğ”¶â‚â‚™â‚[l] = map(Ï†, ğ¯â½Ë¡â¾â‚â‚™â‚)
            l==L && (ğ[n] = eâ‚â‚™â‚ - ğ”¶â‚â‚™â‚[L][1]; ğ²[n]= ğ”¶â‚â‚™â‚[L][1])
        end
    end
    RMSE = âˆš(Î£(ğ.^2)/N)
    return is_output ? (RMSE, ğ²) : RMSE # RMSE of the test dataset
end

## algorithm parameters and hyperparameters
Nâ‚œáµ£â‚™ = 80 # % percentage of instances for the train dataset
Nâ‚œâ‚›â‚œ = 20 # % percentage of instances for the test dataset
Nâ‚ = 1 # number of number of attributes (only x(n))
Náµ£ = 20 # number of realizations
Nâ‚‘ = 100 # number of epochs
mâ‚‚ = 1 # regression problem
Î· = 0.5 # learning step

## Standardize dataset input and output in [-1,1] (Preprocessing)
Î¼ = Î£(ğ)/N # mean
ğ”¼Î¼Â² = Î£(ğ.^2)/N # second moment of
ÏƒÎ¼ = sqrt.(ğ”¼Î¼Â² - Î¼.^2) # standard deviation
ğ = (ğ .- Î¼)./ÏƒÎ¼ # zero mean and unit variance
ğ = ğ./maximum(abs.(ğ))

Î¼ = Î£(ğ±)/N # mean
ğ”¼Î¼Â² = Î£(ğ±.^2)/N # second moment of
ÏƒÎ¼ = sqrt.(ğ”¼Î¼Â² - Î¼.^2) # standard deviation
ğ± = (ğ± .- Î¼)./ÏƒÎ¼ # zero mean and unit variance
ğ± = ğ±./maximum(abs.(ğ±))

## init
ğ›â‚œâ‚›â‚œ = fill(NaN, Náµ£) # vector of accuracies for test dataset
for náµ£ âˆˆ 1:Náµ£
    # prepare the data!
    global ğ±, ğ = shuffle_dataset(ğ±, ğ)
    # hould-out
    ğ±â‚œáµ£â‚™ = ğ±[1:(N*Nâ‚œáµ£â‚™)Ã·100]
    ğâ‚œáµ£â‚™ = ğ[1:(N*Nâ‚œáµ£â‚™)Ã·100]
    ğ±â‚œâ‚›â‚œ = ğ±[length(ğâ‚œáµ£â‚™)+1:end]
    ğâ‚œâ‚›â‚œ = ğ[length(ğâ‚œáµ£â‚™)+1:end]

    # grid search with k-fold cross validation!
    # (mâ‚, (Ï†, Ï†Ê¼, a)) = grid_search_cross_validation(ğ±â‚œáµ£â‚™, ğâ‚œáµ£â‚™, 10, (1:3, ((vâ‚â‚™â‚ -> 1/(1+â„¯^(-vâ‚â‚™â‚)), yâ‚â‚™â‚ -> yâ‚â‚™â‚*(1-yâ‚â‚™â‚), 1), (vâ‚â‚™â‚ -> (1-â„¯^(-vâ‚â‚™â‚))/(1+â„¯^(-vâ‚â‚™â‚)), yâ‚â‚™â‚ -> .5(1-yâ‚â‚™â‚^2), 2))), "RMSE")
    # println("For the realization $(náµ£)")
    # println("best mâ‚: $(mâ‚)")
    # println("best Ï†: $(a==1 ? "logistic" : "Hyperbolic")")
    (mâ‚, (Ï†, Ï†Ê¼, a)) = (7, (vâ‚â‚™â‚ -> (1-â„¯^(-vâ‚â‚™â‚))/(1+â„¯^(-vâ‚â‚™â‚)), yâ‚â‚™â‚ -> .5(1-yâ‚â‚™â‚^2), 1))
    # (mâ‚, (Ï†, Ï†Ê¼, a)) = (7, (vâ‚â‚™â‚ -> 1/(1+â„¯^(-vâ‚â‚™â‚)), yâ‚â‚™â‚ -> yâ‚â‚™â‚*(1-yâ‚â‚™â‚), 1))
    
    # initialize!
    ğ”š = OrderedDict(1 => rand(mâ‚, Nâ‚+1), 2 => rand(5, mâ‚+1), 3 => rand(mâ‚‚, 5+1)) # 1 => first layer (hidden layer) 2 => second layer 
    ğ›â‚œáµ£â‚™ = fill(NaN, Nâ‚‘) # vector of accuracies for train dataset (to see its evolution during training phase)

    # train!
    for nâ‚‘ âˆˆ 1:1 # for each epoch
        ğ”š, ğ›â‚œáµ£â‚™[nâ‚‘] = train(ğ±â‚œáµ£â‚™, ğâ‚œáµ£â‚™, ğ”š, Ï†, Ï†Ê¼)
        ğ±â‚œáµ£â‚™, ğâ‚œáµ£â‚™ = shuffle_dataset(ğ±â‚œáµ£â‚™, ğâ‚œáµ£â‚™)
    end
    # test!
    if náµ£==1
        global ğ›â‚œâ‚›â‚œ[náµ£], ğ² = test(ğ±â‚œâ‚›â‚œ, ğâ‚œâ‚›â‚œ, ğ”š, Ï†, true) # accuracy for this realization
        display(plot([ğ² ğâ‚œâ‚›â‚œ]))
        global ğ”š, Ï†
    else
        global ğ›â‚œâ‚›â‚œ[náµ£] = test(ğ±â‚œâ‚›â‚œ, ğâ‚œâ‚›â‚œ, ğ”š, Ï†) # accuracy for this realization
    end
end

# analyze the accuracy statistics of each independent realization
ğ”¼ğ›â‚œâ‚›â‚œ = Î£(ğ›â‚œâ‚›â‚œ)./Náµ£ # mean
ğ”¼ğ›â‚œâ‚›â‚œÂ² = Î£(ğ›â‚œâ‚›â‚œ.^2)./Náµ£ # second moment
Ïƒğ›â‚œâ‚›â‚œ = sqrt.(ğ”¼ğ›â‚œâ‚›â‚œÂ² .- ğ”¼ğ›â‚œâ‚›â‚œ.^2) # standard deviation

println("* Mean RMSE: $(ğ”¼ğ›â‚œâ‚›â‚œ)")
println("* Standard deviation: $(Ïƒğ›â‚œâ‚›â‚œ)")


# make plot!
ğ± = range(0, 2Ï€, length=N)
ğ = map(xâ‚â‚™â‚ -> 3sin(xâ‚â‚™â‚)+1, ğ±)
## Standardize dataset input and output in [-1,1] (Preprocessing)
Î¼ = Î£(ğ)/N # mean
ğ”¼Î¼Â² = Î£(ğ.^2)/N # second moment of
ÏƒÎ¼ = sqrt.(ğ”¼Î¼Â² - Î¼.^2) # standard deviation
ğ = (ğ .- Î¼)./ÏƒÎ¼ # zero mean and unit variance
ğ = ğ./maximum(abs.(ğ))

Î¼ = Î£(ğ±)/N # mean
ğ”¼Î¼Â² = Î£(ğ±.^2)/N # second moment of
ÏƒÎ¼ = sqrt.(ğ”¼Î¼Â² - Î¼.^2) # standard deviation
ğ± = (ğ± .- Î¼)./ÏƒÎ¼ # zero mean and unit variance
ğ± = ğ±./maximum(abs.(ğ±))
display(plot(ğ±, ğ))

_, ğ² = test(ğ±, ğ, ğ”š, Ï†, true)

fig = plot(ğ±, [ğ² ğ], label=["Output signal" "Desired signal"], linewidth=3)

savefig(fig, "figs/sine_regression.png")