using FileIO, JLD2, Random, LinearAlgebra, Plots, LaTeXStrings
Î£=sum

## load the data
ğ—, labels = FileIO.load("Datasets/Iris [uci]/iris.jld2", "ğ—", "ğ") # ğ— â¡ [attributes X instances]
# PS choose only one!!!
# uncomment â†“ if you want to train for all attributes
ğ— = [fill(-1, size(ğ—,2))'; ğ—] # add the -1 input (bias)
# uncomment â†“ if you want to train for petal length and width (to plot the decision surface)
# ğ— = [fill(-1, size(ğ—,2))'; ğ—[3:4,:]] # add the -1 input (bias)

## useful functions
function shuffle_dataset(ğ—, ğ)
    shuffle_indices = Random.shuffle(1:size(ğ—,2))
    return ğ—[:, shuffle_indices], ğ[shuffle_indices]
end

function train(ğ—, ğ, ğ°, is_training_accuracy=true)
    Ï† = uâ‚â‚™â‚ -> uâ‚â‚™â‚>0 ? 1 : 0 # McCulloch and Pitts's activation function (step function)
    Nâ‚‘ = 0 # number of errors â¡ misclassifications
    for (ğ±â‚â‚™â‚, dâ‚â‚™â‚) âˆˆ zip(eachcol(ğ—), ğ)
        Î¼â‚â‚™â‚ = dot(ğ±â‚â‚™â‚,ğ°) # inner product
        yâ‚â‚™â‚ = Ï†(Î¼â‚â‚™â‚) # for the training phase, you do not pass yâ‚â‚™â‚ to a harder decisor (the McCulloch and Pitts's activation function) since you are in intended to classify yâ‚â‚™â‚. Rather, you are interested in updating ğ° (??? TODO)
        eâ‚â‚™â‚ = dâ‚â‚™â‚ - yâ‚â‚™â‚
        ğ° += Î±*eâ‚â‚™â‚*ğ±â‚â‚™â‚

        # this part is optional: only if it is interested in seeing the accuracy evolution of the training dataset throughout the epochs
        Nâ‚‘ = eâ‚â‚™â‚==0 ? Nâ‚‘ : Nâ‚‘+1
    end
    if is_training_accuracy
        accuracy = (length(ğ)-Nâ‚‘)/length(ğ) # accuracy for this epoch
        return ğ°, accuracy
    else
        return ğ°
    end
end

function test(ğ—, ğ, ğ°, is_confusion_matrix=false)
    Ï† = uâ‚â‚™â‚ -> uâ‚â‚™â‚>0 ? 1 : 0 # McCulloch and Pitts's activation function (step function)
    ğ² = rand(length(ğ)) # vector of predictions for confusion matrix
    Nâ‚‘ = 0
    for (n, (ğ±â‚â‚™â‚, dâ‚â‚™â‚)) âˆˆ enumerate(zip(eachcol(ğ—), ğ))
        Î¼â‚â‚™â‚ = ğ±â‚â‚™â‚â‹…ğ° # inner product
        yâ‚â‚™â‚ = Ï†(Î¼â‚â‚™â‚) # for the single-unit perceptron, yâ‚â‚™â‚ âˆˆ {0,1}. Therefore, it is not necessary to pass yâ‚â‚™â‚ to a harder decisor since Ï†(â‹…) already does this job
        ğ²[n] = yâ‚â‚™â‚

        Nâ‚‘ = yâ‚â‚™â‚==dâ‚â‚™â‚ ? Nâ‚‘ : Nâ‚‘+1
    end
    if !is_confusion_matrix
        accuracy = (length(ğ)-Nâ‚‘)/length(ğ)
        return accuracy # return the accuracy for this realization
    else
        return Int.(ğ²) # return the errors over the instances to plot the confusion matrix
    end
end

## algorithm parameters hyperparameters
Náµ£ = 20 # number of realizations
Nâ‚ = size(ğ—, 1) # =5 (including bias) number of Attributes, that is, input vector size at each intance. They mean: sepal length, sepal width, petal length, petal width
N = size(ğ—, 2) # =150 number of instances(samples)
Nâ‚œáµ£â‚™ = 80 # % percentage of instances for the train dataset
Nâ‚œâ‚›â‚œ = 20 # % percentage of instances for the test dataset
Nâ‚‘ = 100 # number of epochs
Î± = 0.01 # learning step

## init
all_aÌ„cÌ„cÌ„, all_Ïƒacc, all_ğ°â‚’â‚šâ‚œ = rand(3), rand(3), rand(Nâ‚,3)
for (i, desired_label) âˆˆ enumerate(("setosa", "virginica", "versicolor"))
    local ğ = labels.==desired_label # dâ‚â‚™â‚ âˆˆ {0,1}
    local accâ‚œâ‚›â‚œ = fill(NaN, Náµ£) # vector of accuracies for test dataset (to compute the final statistics)
    for náµ£ âˆˆ 1:Náµ£ # for each realization
        # initializing!
        accâ‚œáµ£â‚™ = fill(NaN, Nâ‚‘) # vector of accuracies for train dataset (to see its evolution during training phase)
        global ğ° = ones(Nâ‚) # initialize a new McCulloch-Pitts neuron (a new set of parameters)
        global ğ— # ?

        # prepare the data!
        ğ—, ğ = shuffle_dataset(ğ—, ğ)
        # hould-out
        global ğ—â‚œáµ£â‚™ = ğ—[:,1:(N*Nâ‚œáµ£â‚™)Ã·100]
        global ğâ‚œáµ£â‚™ = ğ[1:(N*Nâ‚œáµ£â‚™)Ã·100]
        global ğ—â‚œâ‚›â‚œ = ğ—[:,length(ğâ‚œáµ£â‚™)+1:end]
        global ğâ‚œâ‚›â‚œ = ğ[length(ğâ‚œáµ£â‚™)+1:end]

        # train!
        for nâ‚‘ âˆˆ 1:Nâ‚‘ # for each epoch
            ğ°, accâ‚œáµ£â‚™[nâ‚‘] = train(ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™, ğ°)
            ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™ = shuffle_dataset(ğ—â‚œáµ£â‚™, ğâ‚œáµ£â‚™)
        end
        # test!
        accâ‚œâ‚›â‚œ[náµ£] = test(ğ—â‚œâ‚›â‚œ, ğâ‚œâ‚›â‚œ, ğ°)
        
        # make plots!
        if náµ£ == 1
            all_ğ°â‚’â‚šâ‚œ[:,i] = ğ° # save the optimum value reached during the 1th realization for setosa, versicolor, and virginica
            # if all attributes was taken into account, compute the accuracyxepochs for all classes
            if length(ğ°) != 3
                local p = plot(accâ‚œáµ£â‚™, label="", xlabel=L"Epochs", ylabel="Accuracy", linewidth=2, title="Training accuracy for $(desired_label) class by epochs")
                display(p)
                savefig(p, "trab1 (single-unit perceptron)/figs/accuracy-by-epochs-for-$(desired_label).png")
                # for the setosa class, compute the confusion matrix
                if desired_label == "setosa"
                    ğ‚ = zeros(2,2) # confusion matrix
                    ğ²â‚œâ‚›â‚œ = test(ğ—â‚œâ‚›â‚œ, ğâ‚œâ‚›â‚œ, ğ°, true)
                    for n âˆˆ 1:length(ğ²â‚œâ‚›â‚œ)
                        # predicted x true label
                        ğ‚[ğ²â‚œâ‚›â‚œ[n]+1, ğâ‚œâ‚›â‚œ[n]+1] += 1
                    end
                    h = heatmap(ğ‚, xlabel="Predicted labels", ylabel="True labels", xticks=(1:2, ("setosa", "not setosa")), yticks=(1:2, ("setosa", "not setosa")), title="Confusion matrix for the setosa class")
                    savefig(h, "trab1 (single-unit perceptron)/figs/setosa-confusion-matrix.png")
                    display(h) # TODO: put the number onto each confusion square
                end
            end
            # decision surface
            if length(ğ°) == 3 # plot the surface only if the learning procedure was taken with only two attributes, the petal length and petal width (equals to 3 because the bias)
                Ï† = uâ‚â‚™â‚ -> uâ‚â‚™â‚â‰¥0 ? 1 : 0 # activation function of the single-unit perceptron
                xâ‚ƒ_range = floor(minimum(ğ—[2,:])):.1:ceil(maximum(ğ—[2,:]))
                xâ‚„_range = floor(minimum(ğ—[3,:])):.1:ceil(maximum(ğ—[3,:]))
                y(xâ‚ƒ, xâ‚„) = Ï†(dot([-1, xâ‚ƒ, xâ‚„], ğ°))
                p = surface(xâ‚ƒ_range, xâ‚„_range, y, camera=(60,40,0), xlabel = "petal length", ylabel = "petal width", zlabel="decision surface")

                # scatter plot for the petal length and petal length width for the setosa class
                # train and desired label 
                scatter!(ğ—â‚œáµ£â‚™[2,ğâ‚œáµ£â‚™.==1], ğ—â‚œáµ£â‚™[3,ğâ‚œáµ£â‚™.==1], ones(length(filter(x->x==1, ğâ‚œáµ£â‚™))),
                        markershape = :hexagon,
                        markersize = 4,
                        markeralpha = 0.6,
                        markercolor = :green,
                        markerstrokewidth = 3,
                        markerstrokealpha = 0.2,
                        markerstrokecolor = :black,
                        xlabel = "petal\nlength",
                        ylabel = "petal width",
                        camera = (60,40,0),
                        label = "$(desired_label) train set")
                
                # test and desired label 
                scatter!(ğ—â‚œâ‚›â‚œ[2,ğâ‚œâ‚›â‚œ.==1], ğ—â‚œâ‚›â‚œ[3,ğâ‚œâ‚›â‚œ.==1], ones(length(filter(x->x==1, ğâ‚œâ‚›â‚œ))),
                        markershape = :cross,
                        markersize = 4,
                        markeralpha = 0.6,
                        markercolor = :green,
                        markerstrokewidth = 3,
                        markerstrokealpha = 0.2,
                        markerstrokecolor = :black,
                        xlabel = "petal\nlength",
                        ylabel = "petal width",
                        camera = (60,40,0),
                        label = "$(desired_label) test set")

                # train and not desired label 
                scatter!(ğ—â‚œáµ£â‚™[2,ğâ‚œáµ£â‚™.==0], ğ—â‚œáµ£â‚™[3,ğâ‚œáµ£â‚™.==0], zeros(length(filter(x->x==0, ğâ‚œáµ£â‚™))),
                        markershape = :hexagon,
                        markersize = 4,
                        markeralpha = 0.6,
                        markercolor = :red,
                        markerstrokewidth = 3,
                        markerstrokealpha = 0.2,
                        markerstrokecolor = :black,
                        xlabel = "petal\nlength",
                        ylabel = "petal width",
                        camera = (60,40,0),
                        label = "not $(desired_label) train set")

                # test and not desired label 
                scatter!(ğ—â‚œâ‚›â‚œ[2,ğâ‚œâ‚›â‚œ.==0], ğ—â‚œâ‚›â‚œ[3,ğâ‚œâ‚›â‚œ.==0], zeros(length(filter(x->x==0, ğâ‚œâ‚›â‚œ))),
                        markershape = :cross,
                        markersize = 4,
                        markeralpha = 0.6,
                        markercolor = :red,
                        markerstrokewidth = 3,
                        markerstrokealpha = 0.2,
                        markerstrokecolor = :black,
                        xlabel = "petal\nlength",
                        ylabel = "petal width",
                        camera = (60,40,0),
                        label = "not $(desired_label) test set")
                
                title!("Decision surface for the class $(desired_label)")
                display(p)
                savefig(p,"trab1 (single-unit perceptron)/figs/decision-surface-for-$(desired_label).png")
            end
        end
    end
    # analyze the accuracy statistics of each independent realization
    local aÌ„cÌ„cÌ„ = Î£(accâ‚œâ‚›â‚œ)/Náµ£ # Mean
    local ğ”¼accÂ² = Î£(accâ‚œâ‚›â‚œ.^2)/Náµ£
    local Ïƒacc = sqrt.(ğ”¼accÂ² .- aÌ„cÌ„cÌ„.^2) # standard deviation
    
    # save the performance
    all_aÌ„cÌ„cÌ„[i] = aÌ„cÌ„cÌ„
    all_Ïƒacc[i] = Ïƒacc
    println("Mean accuracy for $(desired_label): $(aÌ„cÌ„cÌ„)")
    println("Standard deviation for $(desired_label): $(Ïƒacc)")
end